{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comprehensive Evaluation Suite\n",
        "\n",
        "This notebook provides comprehensive evaluation tools for analyzing model performance across different domains.\n",
        "\n",
        "## Features:\n",
        "1. **Per-Class Metrics**: F1, precision, recall for each class in each domain\n",
        "2. **Confusion Matrices**: Visual confusion matrices for each domain\n",
        "3. **Feature Visualization**: t-SNE/UMAP visualization of feature space\n",
        "4. **Domain Classification**: Analyze domain classification accuracy\n",
        "5. **Failure Analysis**: Detailed analysis of misclassifications\n",
        "6. **Comparison Tools**: Compare multiple models side-by-side\n",
        "\n",
        "## Usage:\n",
        "Load a trained model and evaluation results, then run the analysis cells.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 39 classes\n",
            "Base directory: d:\\Programming\\Seminar_Project\\PLANT_LEAF_DISEASE_DETECTION\n"
          ]
        }
      ],
      "source": [
        "# Imports and Setup\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, \n",
        "    f1_score, precision_score, recall_score\n",
        ")\n",
        "from sklearn.manifold import TSNE\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Paths\n",
        "current_dir = Path(os.getcwd())\n",
        "if current_dir.name == \"experiment_2\":\n",
        "    BASE_DIR = current_dir.parent\n",
        "else:\n",
        "    BASE_DIR = current_dir\n",
        "\n",
        "METADATA_DIR = BASE_DIR / \"metadata\"\n",
        "LABEL_MAPPING_PATH = METADATA_DIR / \"label_mapping.json\"\n",
        "MODELS_DIR = BASE_DIR / \"models\"\n",
        "\n",
        "# Load label mappings\n",
        "with open(LABEL_MAPPING_PATH, \"r\") as f:\n",
        "    label_mapping = json.load(f)\n",
        "\n",
        "id_to_label = {c[\"id\"]: c[\"canonical_label\"] for c in label_mapping[\"classes\"]}\n",
        "label_to_id = {v: k for k, v in id_to_label.items()}\n",
        "num_classes = len(label_mapping[\"classes\"])\n",
        "\n",
        "print(f\"Loaded {num_classes} classes\")\n",
        "print(f\"Base directory: {BASE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 1: Per-Class Metrics Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Per-class metrics functions defined\n"
          ]
        }
      ],
      "source": [
        "def compute_per_class_metrics(targets, preds, id_to_label):\n",
        "    \"\"\"Compute per-class F1, precision, and recall.\"\"\"\n",
        "    class_report = classification_report(\n",
        "        targets, preds,\n",
        "        labels=list(range(len(id_to_label))),\n",
        "        target_names=[id_to_label[i] for i in range(len(id_to_label))],\n",
        "        output_dict=True,\n",
        "        zero_division=0\n",
        "    )\n",
        "    \n",
        "    metrics = []\n",
        "    for class_id in range(len(id_to_label)):\n",
        "        class_name = id_to_label[class_id]\n",
        "        if class_name in class_report:\n",
        "            metrics.append({\n",
        "                'class_id': class_id,\n",
        "                'class_name': class_name,\n",
        "                'precision': class_report[class_name]['precision'],\n",
        "                'recall': class_report[class_name]['recall'],\n",
        "                'f1': class_report[class_name]['f1-score'],\n",
        "                'support': class_report[class_name]['support']\n",
        "            })\n",
        "        else:\n",
        "            metrics.append({\n",
        "                'class_id': class_id,\n",
        "                'class_name': class_name,\n",
        "                'precision': 0.0,\n",
        "                'recall': 0.0,\n",
        "                'f1': 0.0,\n",
        "                'support': 0\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(metrics)\n",
        "\n",
        "def print_per_class_summary(df_metrics, dataset_name):\n",
        "    \"\"\"Print summary of per-class metrics.\"\"\"\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"PER-CLASS METRICS: {dataset_name.upper()}\")\n",
        "    print(f\"{'='*70}\")\n",
        "    \n",
        "    # Filter classes with support > 0\n",
        "    df_active = df_metrics[df_metrics['support'] > 0].copy()\n",
        "    \n",
        "    if len(df_active) == 0:\n",
        "        print(\"No samples found for any class.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\nTotal classes with samples: {len(df_active)}\")\n",
        "    print(f\"Average F1: {df_active['f1'].mean():.4f}\")\n",
        "    print(f\"Average Precision: {df_active['precision'].mean():.4f}\")\n",
        "    print(f\"Average Recall: {df_active['recall'].mean():.4f}\")\n",
        "    \n",
        "    print(\"\\nTop 10 Best Performing Classes:\")\n",
        "    top10 = df_active.nlargest(10, 'f1')[['class_name', 'f1', 'precision', 'recall', 'support']]\n",
        "    print(top10.to_string(index=False))\n",
        "    \n",
        "    print(\"\\nTop 10 Worst Performing Classes:\")\n",
        "    worst10 = df_active.nsmallest(10, 'f1')[['class_name', 'f1', 'precision', 'recall', 'support']]\n",
        "    print(worst10.to_string(index=False))\n",
        "\n",
        "print(\"✓ Per-class metrics functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 2: Confusion Matrix Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Confusion matrix visualization function defined\n"
          ]
        }
      ],
      "source": [
        "def plot_confusion_matrix(targets, preds, id_to_label, dataset_name, save_path=None):\n",
        "    \"\"\"Plot confusion matrix with class labels.\"\"\"\n",
        "    cm = confusion_matrix(targets, preds, labels=list(range(len(id_to_label))))\n",
        "    \n",
        "    # Get class names for active classes only\n",
        "    active_classes = [i for i in range(len(id_to_label)) if cm[i].sum() > 0 or cm[:, i].sum() > 0]\n",
        "    if len(active_classes) == 0:\n",
        "        print(f\"No active classes found for {dataset_name}\")\n",
        "        return None\n",
        "    \n",
        "    # Filter confusion matrix to active classes\n",
        "    cm_filtered = cm[np.ix_(active_classes, active_classes)]\n",
        "    class_names = [id_to_label[i] for i in active_classes]\n",
        "    \n",
        "    # Normalize by row (true class)\n",
        "    cm_normalized = cm_filtered.astype('float') / (cm_filtered.sum(axis=1)[:, np.newaxis] + 1e-8)\n",
        "    \n",
        "    # Create figure\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
        "    \n",
        "    # Plot raw counts\n",
        "    sns.heatmap(cm_filtered, annot=True, fmt='d', cmap='Blues', \n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                ax=ax1, cbar_kws={'label': 'Count'})\n",
        "    ax1.set_title(f'Confusion Matrix (Counts) - {dataset_name}', fontsize=14, fontweight='bold')\n",
        "    ax1.set_xlabel('Predicted', fontsize=12)\n",
        "    ax1.set_ylabel('True', fontsize=12)\n",
        "    plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')\n",
        "    plt.setp(ax1.get_yticklabels(), rotation=0)\n",
        "    \n",
        "    # Plot normalized\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                xticklabels=class_names, yticklabels=class_names,\n",
        "                ax=ax2, cbar_kws={'label': 'Normalized'})\n",
        "    ax2.set_title(f'Confusion Matrix (Normalized) - {dataset_name}', fontsize=14, fontweight='bold')\n",
        "    ax2.set_xlabel('Predicted', fontsize=12)\n",
        "    ax2.set_ylabel('True', fontsize=12)\n",
        "    plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')\n",
        "    plt.setp(ax2.get_yticklabels(), rotation=0)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Saved confusion matrix to: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "print(\"✓ Confusion matrix visualization function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 3: Feature Space Visualization (t-SNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Feature visualization function defined\n"
          ]
        }
      ],
      "source": [
        "def visualize_features_tsne(model, dataloader, id_to_label, dataset_name, device='cuda', n_samples=1000, save_path=None):\n",
        "    \"\"\"Visualize feature space using t-SNE.\"\"\"\n",
        "    model.eval()\n",
        "    features_list = []\n",
        "    labels_list = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, targets) in enumerate(dataloader):\n",
        "            if len(features_list) * dataloader.batch_size >= n_samples:\n",
        "                break\n",
        "            \n",
        "            images = images.to(device)\n",
        "            targets = targets.cpu().numpy()\n",
        "            \n",
        "            # Extract features (before classifier)\n",
        "            # For EfficientNet/ViT, we need to get features from the model\n",
        "            # This is model-specific - adjust based on your model architecture\n",
        "            try:\n",
        "                # Try to get features from model\n",
        "                if hasattr(model, 'forward_features'):\n",
        "                    features = model.forward_features(images)\n",
        "                elif hasattr(model, 'forward_head'):\n",
        "                    # For timm models, get features before classifier\n",
        "                    features = model.forward_features(images)\n",
        "                    features = model.global_pool(features)\n",
        "                else:\n",
        "                    # Fallback: use penultimate layer\n",
        "                    # This requires model modification or hook\n",
        "                    print(\"Warning: Cannot extract features automatically. Skipping t-SNE.\")\n",
        "                    return None\n",
        "                \n",
        "                # Flatten features\n",
        "                features = features.cpu().numpy().reshape(features.size(0), -1)\n",
        "                \n",
        "                features_list.append(features)\n",
        "                labels_list.append(targets)\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting features: {e}\")\n",
        "                return None\n",
        "    \n",
        "    if len(features_list) == 0:\n",
        "        print(\"No features extracted.\")\n",
        "        return None\n",
        "    \n",
        "    # Concatenate all features\n",
        "    all_features = np.vstack(features_list)\n",
        "    all_labels = np.concatenate(labels_list)\n",
        "    \n",
        "    # Limit to n_samples\n",
        "    if len(all_features) > n_samples:\n",
        "        indices = np.random.choice(len(all_features), n_samples, replace=False)\n",
        "        all_features = all_features[indices]\n",
        "        all_labels = all_labels[indices]\n",
        "    \n",
        "    # Apply t-SNE\n",
        "    print(f\"Applying t-SNE to {len(all_features)} samples...\")\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "    features_2d = tsne.fit_transform(all_features)\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], \n",
        "                         c=all_labels, cmap='tab20', alpha=0.6, s=20)\n",
        "    plt.colorbar(scatter, label='Class ID')\n",
        "    plt.title(f'Feature Space Visualization (t-SNE) - {dataset_name}', fontsize=14, fontweight='bold')\n",
        "    plt.xlabel('t-SNE Component 1', fontsize=12)\n",
        "    plt.ylabel('t-SNE Component 2', fontsize=12)\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Saved t-SNE visualization to: {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    return features_2d, all_labels\n",
        "\n",
        "print(\"✓ Feature visualization function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function 4: Failure Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Failure analysis function defined\n"
          ]
        }
      ],
      "source": [
        "def analyze_failures(targets, preds, id_to_label, dataset_name, top_n=10):\n",
        "    \"\"\"Analyze failure patterns and misclassifications.\"\"\"\n",
        "    cm = confusion_matrix(targets, preds, labels=list(range(len(id_to_label))))\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"FAILURE ANALYSIS: {dataset_name.upper()}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    # Per-class accuracy\n",
        "    print(\"Per-Class Accuracy:\")\n",
        "    for class_id in range(len(id_to_label)):\n",
        "        if cm[class_id].sum() > 0:\n",
        "            acc = cm[class_id, class_id] / cm[class_id].sum()\n",
        "            class_name = id_to_label[class_id]\n",
        "            print(f\"  Class {class_id} ({class_name}): {acc:.3f}\")\n",
        "    \n",
        "    # Most common misclassifications\n",
        "    print(f\"\\nTop {top_n} Misclassifications:\")\n",
        "    misclass_pairs = []\n",
        "    for i in range(len(cm)):\n",
        "        for j in range(len(cm)):\n",
        "            if i != j and cm[i, j] > 0:\n",
        "                misclass_pairs.append((i, j, cm[i, j]))\n",
        "    misclass_pairs.sort(key=lambda x: x[2], reverse=True)\n",
        "    \n",
        "    for i, j, count in misclass_pairs[:top_n]:\n",
        "        true_name = id_to_label[i]\n",
        "        pred_name = id_to_label[j]\n",
        "        print(f\"  {true_name} → {pred_name}: {count} times\")\n",
        "    \n",
        "    # Overall statistics\n",
        "    total_samples = len(targets)\n",
        "    correct = (targets == preds).sum()\n",
        "    accuracy = correct / total_samples if total_samples > 0 else 0.0\n",
        "    \n",
        "    print(f\"\\nOverall Statistics:\")\n",
        "    print(f\"  Total samples: {total_samples}\")\n",
        "    print(f\"  Correct predictions: {correct}\")\n",
        "    print(f\"  Incorrect predictions: {total_samples - correct}\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "    \n",
        "    return misclass_pairs\n",
        "\n",
        "print(\"✓ Failure analysis function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Example\n",
        "\n",
        "To use this evaluation suite:\n",
        "\n",
        "1. Load your model and evaluation results (from previous notebooks)\n",
        "2. Run the analysis functions on your results\n",
        "3. Example:\n",
        "   ```python\n",
        "   # Assuming you have results from evaluate_on_all_datasets\n",
        "   results = {...}  # Your evaluation results\n",
        "   \n",
        "   # Per-class metrics\n",
        "   df_metrics = compute_per_class_metrics(\n",
        "       results['main']['all_targets'],\n",
        "       results['main']['all_preds'],\n",
        "       id_to_label\n",
        "   )\n",
        "   print_per_class_summary(df_metrics, 'main')\n",
        "   \n",
        "   # Confusion matrix\n",
        "   plot_confusion_matrix(\n",
        "       results['main']['all_targets'],\n",
        "       results['main']['all_preds'],\n",
        "       id_to_label,\n",
        "       'main',\n",
        "       save_path='confusion_matrix_main.png'\n",
        "   )\n",
        "   \n",
        "   # Failure analysis\n",
        "   analyze_failures(\n",
        "       results['plant_doc']['all_targets'],\n",
        "       results['plant_doc']['all_preds'],\n",
        "       id_to_label,\n",
        "       'plant_doc'\n",
        "   )\n",
        "   ```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
