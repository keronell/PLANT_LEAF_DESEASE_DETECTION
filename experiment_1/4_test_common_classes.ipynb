{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test: Training on Common Classes Across All Three Datasets\n",
        "\n",
        "This notebook implements:\n",
        "1. **Test 1**: Training both EfficientNet and ViT on a collective dataset containing only classes that exist in ALL THREE datasets (Main, Plant_doc, and FieldPlant)\n",
        "2. **Validation Block**: Performance statistics on validation images from all three datasets separately\n",
        "\n",
        "## Common Classes Identified:\n",
        "- Corn___Cercospora_leaf_spot Gray_leaf_spot\n",
        "- Corn___Common_rust\n",
        "- Corn___Northern_Leaf_Blight\n",
        "- Tomato___healthy\n",
        "- Tomato___Tomato_mosaic_virus\n",
        "- Tomato___Tomato_Yellow_Leaf_Curl_Virus\n",
        "\n",
        "**Total: 6 common classes**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Imports and Setup\n",
        "import os\n",
        "import time\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import timm\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# Paths\n",
        "METADATA_DIR = Path(\"./metadata\")\n",
        "LABEL_MAPPING_PATH = METADATA_DIR / \"label_mapping.json\"\n",
        "DATASET_INDEX_PATH = METADATA_DIR / \"dataset_index.json\"\n",
        "DATA_DIR = Path(\"./data\")\n",
        "PLANT_DOC_TRAIN = DATA_DIR / \"Plant_doc\" / \"train\"\n",
        "PLANT_DOC_TEST = DATA_DIR / \"Plant_doc\" / \"test\"\n",
        "FIELDPLANT_DIR = DATA_DIR / \"FieldPlant_reformatted\"\n",
        "\n",
        "MODELS_DIR = Path(\"./models\")\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Identify Common Classes Across All Three Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Main dataset: 39 classes\n",
            "\n",
            "Common classes found: 6\n",
            "Common classes:\n",
            "  0: corn_cercospora_leaf_spot_gray_leaf_spot (original ID: 8)\n",
            "  1: corn_common_rust (original ID: 9)\n",
            "  2: corn_northern_leaf_blight (original ID: 11)\n",
            "  3: tomato_healthy (original ID: 31)\n",
            "  4: tomato_tomato_mosaic_virus (original ID: 37)\n",
            "  5: tomato_tomato_yellow_leaf_curl_virus (original ID: 38)\n"
          ]
        }
      ],
      "source": [
        "# Load main dataset metadata\n",
        "with open(LABEL_MAPPING_PATH, \"r\") as f:\n",
        "    label_mapping = json.load(f)\n",
        "\n",
        "with open(DATASET_INDEX_PATH, \"r\") as f:\n",
        "    dataset_index = json.load(f)\n",
        "\n",
        "# Create mapping from main dataset\n",
        "id_to_label = {c[\"id\"]: c[\"canonical_label\"] for c in label_mapping[\"classes\"]}\n",
        "label_to_id = {v: k for k, v in id_to_label.items()}\n",
        "folder_to_label = {}\n",
        "for c in label_mapping[\"classes\"]:\n",
        "    for folder in c.get(\"pv_folders\", []):\n",
        "        folder_to_label[folder] = c[\"canonical_label\"]\n",
        "\n",
        "print(f\"Main dataset: {len(label_mapping['classes'])} classes\")\n",
        "\n",
        "# Define common classes (classes that exist in ALL THREE datasets)\n",
        "# Based on dataset_classes_diagram.txt analysis\n",
        "COMMON_CLASSES_MAIN = [\n",
        "    \"Corn___Cercospora_leaf_spot Gray_leaf_spot\",  # ID 8\n",
        "    \"Corn___Common_rust\",                          # ID 9\n",
        "    \"Corn___Northern_Leaf_Blight\",                 # ID 11\n",
        "    \"Tomato___healthy\",                            # ID 31\n",
        "    \"Tomato___Tomato_mosaic_virus\",               # ID 37\n",
        "    \"Tomato___Tomato_Yellow_Leaf_Curl_Virus\",     # ID 38\n",
        "]\n",
        "\n",
        "# Map to canonical labels\n",
        "common_canonical_labels = []\n",
        "common_class_ids_original = []\n",
        "for folder_name in COMMON_CLASSES_MAIN:\n",
        "    canonical = folder_to_label.get(folder_name)\n",
        "    if canonical:\n",
        "        common_canonical_labels.append(canonical)\n",
        "        common_class_ids_original.append(label_to_id[canonical])\n",
        "\n",
        "# Create new label mapping for common classes (0-5)\n",
        "common_id_to_original_id = {new_id: orig_id for new_id, orig_id in enumerate(common_class_ids_original)}\n",
        "original_id_to_common_id = {orig_id: new_id for new_id, orig_id in enumerate(common_class_ids_original)}\n",
        "\n",
        "print(f\"\\nCommon classes found: {len(common_canonical_labels)}\")\n",
        "print(\"Common classes:\")\n",
        "for i, (orig_id, canonical) in enumerate(zip(common_class_ids_original, common_canonical_labels)):\n",
        "    print(f\"  {i}: {canonical} (original ID: {orig_id})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Mapping for Plant_doc and FieldPlant Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plant_doc mapping:\n",
            "  Corn_Gray_leaf_spot -> corn_cercospora_leaf_spot_gray_leaf_spot\n",
            "  Corn_rust_leaf -> corn_common_rust\n",
            "  Corn_leaf_blight -> corn_northern_leaf_blight\n",
            "  Tomato_leaf -> tomato_healthy\n",
            "  Tomato_leaf_mosaic_virus -> tomato_tomato_mosaic_virus\n",
            "  Tomato_leaf_yellow_virus -> tomato_tomato_yellow_leaf_curl_virus\n",
            "\n",
            "FieldPlant mapping:\n",
            "  Corn___Gray_leaf_spot -> corn_cercospora_leaf_spot_gray_leaf_spot\n",
            "  Corn___rust_leaf -> corn_common_rust\n",
            "  Corn___leaf_blight -> corn_northern_leaf_blight\n",
            "  Tomato___healthy -> tomato_healthy\n",
            "  Tomato___leaf_mosaic_virus -> tomato_tomato_mosaic_virus\n",
            "  Tomato___leaf_yellow_virus -> tomato_tomato_yellow_leaf_curl_virus\n"
          ]
        }
      ],
      "source": [
        "# Mapping from Plant_doc folder names to canonical labels\n",
        "plant_doc_to_canonical = {\n",
        "    \"Corn_Gray_leaf_spot\": \"corn_cercospora_leaf_spot_gray_leaf_spot\",\n",
        "    \"Corn_rust_leaf\": \"corn_common_rust\",\n",
        "    \"Corn_leaf_blight\": \"corn_northern_leaf_blight\",\n",
        "    \"Tomato_leaf\": \"tomato_healthy\",\n",
        "    \"Tomato_leaf_mosaic_virus\": \"tomato_tomato_mosaic_virus\",\n",
        "    \"Tomato_leaf_yellow_virus\": \"tomato_tomato_yellow_leaf_curl_virus\",\n",
        "}\n",
        "\n",
        "# Mapping from FieldPlant folder names to canonical labels\n",
        "fieldplant_to_canonical = {\n",
        "    \"Corn___Gray_leaf_spot\": \"corn_cercospora_leaf_spot_gray_leaf_spot\",\n",
        "    \"Corn___rust_leaf\": \"corn_common_rust\",\n",
        "    \"Corn___leaf_blight\": \"corn_northern_leaf_blight\",\n",
        "    \"Corn___healthy\": \"corn_healthy\",  # Note: this is NOT in common classes\n",
        "    \"Tomato___healthy\": \"tomato_healthy\",\n",
        "    \"Tomato___leaf_mosaic_virus\": \"tomato_tomato_mosaic_virus\",\n",
        "    \"Tomato___leaf_yellow_virus\": \"tomato_tomato_yellow_leaf_curl_virus\",\n",
        "}\n",
        "\n",
        "print(\"Plant_doc mapping:\")\n",
        "for folder, canonical in plant_doc_to_canonical.items():\n",
        "    if canonical in common_canonical_labels:\n",
        "        print(f\"  {folder} -> {canonical}\")\n",
        "\n",
        "print(\"\\nFieldPlant mapping:\")\n",
        "for folder, canonical in fieldplant_to_canonical.items():\n",
        "    if canonical in common_canonical_labels:\n",
        "        print(f\"  {folder} -> {canonical}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Filtered Dataset with Only Common Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original training samples: 49179\n",
            "Filtered training samples (common classes only): 8910\n",
            "\n",
            "Class distribution in filtered training set:\n",
            "  Class 0 (corn_cercospora_leaf_spot_gray_leaf_spot): 800 samples\n",
            "  Class 1 (corn_common_rust): 953 samples\n",
            "  Class 2 (corn_northern_leaf_blight): 800 samples\n",
            "  Class 3 (tomato_healthy): 1272 samples\n",
            "  Class 4 (tomato_tomato_mosaic_virus): 800 samples\n",
            "  Class 5 (tomato_tomato_yellow_leaf_curl_virus): 4285 samples\n"
          ]
        }
      ],
      "source": [
        "# Filter main dataset to only common classes\n",
        "def filter_main_dataset(dataset_index, common_class_ids):\n",
        "    \"\"\"Filter main dataset entries to only include common classes.\"\"\"\n",
        "    filtered = []\n",
        "    for entry in dataset_index:\n",
        "        if entry[\"class_id\"] in common_class_ids:\n",
        "            # Create new entry with remapped class_id\n",
        "            new_entry = entry.copy()\n",
        "            new_entry[\"class_id\"] = original_id_to_common_id[entry[\"class_id\"]]\n",
        "            new_entry[\"dataset\"] = \"main\"\n",
        "            filtered.append(new_entry)\n",
        "    return filtered\n",
        "\n",
        "# Filter training data\n",
        "train_entries_main = [e for e in dataset_index if e[\"split\"] == \"train\"]\n",
        "train_entries_filtered = filter_main_dataset(train_entries_main, common_class_ids_original)\n",
        "\n",
        "print(f\"Original training samples: {len(train_entries_main)}\")\n",
        "print(f\"Filtered training samples (common classes only): {len(train_entries_filtered)}\")\n",
        "print(f\"\\nClass distribution in filtered training set:\")\n",
        "class_counts = Counter(e[\"class_id\"] for e in train_entries_filtered)\n",
        "for class_id, count in sorted(class_counts.items()):\n",
        "    canonical = common_canonical_labels[class_id]\n",
        "    print(f\"  Class {class_id} ({canonical}): {count} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Add Plant_doc and FieldPlant Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plant_doc training samples: 0\n",
            "FieldPlant training samples: 4336\n",
            "\n",
            "Total combined training samples: 13246\n",
            "\n",
            "Training samples by dataset:\n",
            "  fieldplant: 4336\n",
            "  main: 8910\n",
            "\n",
            "Final class distribution:\n",
            "  Class 0 (corn_cercospora_leaf_spot_gray_leaf_spot): 908 samples\n",
            "  Class 1 (corn_common_rust): 1051 samples\n",
            "  Class 2 (corn_northern_leaf_blight): 4104 samples\n",
            "  Class 3 (tomato_healthy): 1830 samples\n",
            "  Class 4 (tomato_tomato_mosaic_virus): 838 samples\n",
            "  Class 5 (tomato_tomato_yellow_leaf_curl_virus): 4515 samples\n"
          ]
        }
      ],
      "source": [
        "# Add Plant_doc training data\n",
        "def load_plant_doc_data(data_dir, split=\"train\"):\n",
        "    \"\"\"Load Plant_doc dataset entries.\"\"\"\n",
        "    entries = []\n",
        "    split_dir = data_dir / split\n",
        "    \n",
        "    if not split_dir.exists():\n",
        "        print(f\"Warning: {split_dir} does not exist!\")\n",
        "        return entries\n",
        "    \n",
        "    print(f\"Loading Plant_doc data from {split_dir}\")\n",
        "    print(f\"Available folders: {[f.name for f in split_dir.iterdir() if f.is_dir()]}\")\n",
        "    print(f\"Mapping keys: {list(plant_doc_to_canonical.keys())}\")\n",
        "    \n",
        "    for folder in split_dir.iterdir():\n",
        "        if not folder.is_dir():\n",
        "            continue\n",
        "        \n",
        "        canonical = plant_doc_to_canonical.get(folder.name)\n",
        "        if canonical is None:\n",
        "            print(f\"  Warning: No mapping found for folder '{folder.name}'\")\n",
        "            continue\n",
        "            \n",
        "        if canonical not in common_canonical_labels:\n",
        "            print(f\"  Warning: Canonical label '{canonical}' not in common classes\")\n",
        "            continue\n",
        "            \n",
        "        # Get common class ID\n",
        "        common_class_id = common_canonical_labels.index(canonical)\n",
        "        \n",
        "        # Get all images in folder\n",
        "        image_files = list(folder.glob(\"*.jpg\")) + list(folder.glob(\"*.JPG\"))\n",
        "        print(f\"  Found {len(image_files)} images in {folder.name} -> {canonical} (class_id={common_class_id})\")\n",
        "        for img_path in image_files:\n",
        "            entries.append({\n",
        "                \"path\": str(img_path),\n",
        "                \"class_id\": common_class_id,\n",
        "                \"dataset\": \"plant_doc\",\n",
        "                \"split\": split\n",
        "            })\n",
        "    \n",
        "    return entries\n",
        "\n",
        "# Add FieldPlant training dataInitial attempt at domain adaptation through fine-tuning.\n",
        "def load_fieldplant_data(data_dir):\n",
        "    \"\"\"Load FieldPlant dataset entries.\"\"\"\n",
        "    entries = []\n",
        "    \n",
        "    if not data_dir.exists():\n",
        "        return entries\n",
        "    \n",
        "    for folder in data_dir.iterdir():\n",
        "        if not folder.is_dir():\n",
        "            continue\n",
        "        \n",
        "        canonical = fieldplant_to_canonical.get(folder.name)\n",
        "        if canonical and canonical in common_canonical_labels:\n",
        "            # Get common class ID\n",
        "            common_class_id = common_canonical_labels.index(canonical)\n",
        "            \n",
        "            # Get all images in folder\n",
        "            image_files = list(folder.glob(\"*.jpg\")) + list(folder.glob(\"*.JPG\"))\n",
        "            for img_path in image_files:\n",
        "                entries.append({\n",
        "                    \"path\": str(img_path),\n",
        "                    \"class_id\": common_class_id,\n",
        "                    \"dataset\": \"fieldplant\",\n",
        "                    \"split\": \"train\"  # FieldPlant doesn't have split, use all as train\n",
        "                })\n",
        "    \n",
        "    return entries\n",
        "\n",
        "# Load additional datasets\n",
        "train_entries_plant_doc = load_plant_doc_data(PLANT_DOC_TRAIN, split=\"train\")\n",
        "train_entries_fieldplant = load_fieldplant_data(FIELDPLANT_DIR)\n",
        "\n",
        "print(f\"Plant_doc training samples: {len(train_entries_plant_doc)}\")\n",
        "print(f\"FieldPlant training samples: {len(train_entries_fieldplant)}\")\n",
        "\n",
        "# Combine all training data\n",
        "all_train_entries = train_entries_filtered + train_entries_plant_doc + train_entries_fieldplant\n",
        "print(f\"\\nTotal combined training samples: {len(all_train_entries)}\")\n",
        "\n",
        "# Show distribution by dataset\n",
        "dataset_counts = Counter(e[\"dataset\"] for e in all_train_entries)\n",
        "print(\"\\nTraining samples by dataset:\")\n",
        "for dataset, count in sorted(dataset_counts.items()):\n",
        "    print(f\"  {dataset}: {count}\")\n",
        "\n",
        "# Show distribution by class\n",
        "print(\"\\nFinal class distribution:\")\n",
        "final_class_counts = Counter(e[\"class_id\"] for e in all_train_entries)\n",
        "for class_id, count in sorted(final_class_counts.items()):\n",
        "    canonical = common_canonical_labels[class_id]\n",
        "    print(f\"  Class {class_id} ({canonical}): {count} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create Validation Sets from All Three Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Main dataset validation samples: 1114\n",
            "Plant_doc validation samples: 0\n",
            "FieldPlant validation samples: 867\n"
          ]
        }
      ],
      "source": [
        "# Create validation sets from each dataset\n",
        "val_entries_main = [e for e in dataset_index if e[\"split\"] == \"val\"]\n",
        "val_entries_main_filtered = filter_main_dataset(val_entries_main, common_class_ids_original)\n",
        "\n",
        "val_entries_plant_doc = load_plant_doc_data(PLANT_DOC_TEST, split=\"test\")  # Use test as validation\n",
        "val_entries_fieldplant = load_fieldplant_data(FIELDPLANT_DIR)  # Use all as validation\n",
        "\n",
        "# For FieldPlant, we'll use a subset for validation (e.g., 20% of each class)\n",
        "# For now, use all as validation\n",
        "np.random.seed(42)\n",
        "if len(val_entries_fieldplant) > 0:\n",
        "    indices = np.arange(len(val_entries_fieldplant))\n",
        "    np.random.shuffle(indices)\n",
        "    val_size = int(len(val_entries_fieldplant) * 0.2)\n",
        "    val_entries_fieldplant = [val_entries_fieldplant[i] for i in indices[:val_size]]\n",
        "\n",
        "print(f\"Main dataset validation samples: {len(val_entries_main_filtered)}\")\n",
        "print(f\"Plant_doc validation samples: {len(val_entries_plant_doc)}\")\n",
        "print(f\"FieldPlant validation samples: {len(val_entries_fieldplant)}\")\n",
        "\n",
        "# Store separately for per-dataset evaluation\n",
        "validation_sets = {\n",
        "    \"main\": val_entries_main_filtered,\n",
        "    \"plant_doc\": val_entries_plant_doc,\n",
        "    \"fieldplant\": val_entries_fieldplant\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Define Dataset Class and Transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Transforms\n",
        "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
        "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
        "IMG_SIZE = 224\n",
        "\n",
        "transform_train = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(degrees=20),\n",
        "    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.3, hue=0.1),\n",
        "    T.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
        "    T.ToTensor(),\n",
        "    T.RandomErasing(p=0.3, scale=(0.02, 0.15), ratio=(0.3, 3.3)),\n",
        "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "])\n",
        "\n",
        "transform_eval = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
        "])\n",
        "\n",
        "# Dataset class\n",
        "class CommonClassDataset(Dataset):\n",
        "    def __init__(self, entries, transform_train=True):\n",
        "        self.entries = entries\n",
        "        self.transform_train = transform_train\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.entries)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.entries[idx]\n",
        "        img_path = item[\"path\"]\n",
        "        class_id = item[\"class_id\"]\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {img_path}: {e}\")\n",
        "            # Return a black image as fallback\n",
        "            img = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
        "            class_id = 0\n",
        "\n",
        "        if self.transform_train:\n",
        "            img = transform_train(img)\n",
        "        else:\n",
        "            img = transform_eval(img)\n",
        "\n",
        "        return img, class_id\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = CommonClassDataset(all_train_entries, transform_train=True)\n",
        "val_dataset_main = CommonClassDataset(val_entries_main_filtered, transform_train=False)\n",
        "val_dataset_plant_doc = CommonClassDataset(val_entries_plant_doc, transform_train=False)\n",
        "val_dataset_fieldplant = CommonClassDataset(val_entries_fieldplant, transform_train=False)\n",
        "\n",
        "print(\"Datasets created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Create DataLoaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train batches: 414\n",
            "Main validation batches: 35\n",
            "Plant_doc validation batches: 0\n",
            "FieldPlant validation batches: 28\n"
          ]
        }
      ],
      "source": [
        "# Create weighted sampler for training\n",
        "train_class_counts = Counter(e[\"class_id\"] for e in all_train_entries)\n",
        "max_count = max(train_class_counts.values())\n",
        "class_weights = {cid: max_count / cnt for cid, cnt in train_class_counts.items()}\n",
        "sample_weights = [class_weights[e[\"class_id\"]] for e in all_train_entries]\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=torch.DoubleTensor(sample_weights),\n",
        "    num_samples=len(sample_weights),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    sampler=sampler,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_loader_main = DataLoader(\n",
        "    val_dataset_main,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_loader_plant_doc = DataLoader(\n",
        "    val_dataset_plant_doc,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_loader_fieldplant = DataLoader(\n",
        "    val_dataset_fieldplant,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Main validation batches: {len(val_loader_main)}\")\n",
        "print(f\"Plant_doc validation batches: {len(val_loader_plant_doc)}\")\n",
        "print(f\"FieldPlant validation batches: {len(val_loader_fieldplant)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_preds = []\n",
        "\n",
        "    for images, targets in loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        all_targets.append(targets.detach().cpu())\n",
        "        all_preds.append(preds.detach().cpu())\n",
        "\n",
        "    # Handle empty loader case\n",
        "    if len(all_targets) == 0:\n",
        "        raise ValueError(\"Training loader is empty! Cannot train on empty dataset.\")\n",
        "    \n",
        "    all_targets = torch.cat(all_targets).numpy()\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_acc = (all_targets == all_preds).mean()\n",
        "    epoch_f1 = f1_score(all_targets, all_preds, average=\"macro\")\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_targets = []\n",
        "    all_preds = []\n",
        "\n",
        "    for images, targets in loader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        all_targets.append(targets.detach().cpu())\n",
        "        all_preds.append(preds.detach().cpu())\n",
        "\n",
        "    # Handle empty loader case\n",
        "    if len(all_targets) == 0:\n",
        "        # Return default values for empty validation set\n",
        "        return 0.0, 0.0, 0.0, np.array([]), np.array([])\n",
        "    \n",
        "    all_targets = torch.cat(all_targets).numpy()\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset) if len(loader.dataset) > 0 else 0.0\n",
        "    epoch_acc = (all_targets == all_preds).mean() if len(all_targets) > 0 else 0.0\n",
        "    epoch_f1 = f1_score(all_targets, all_preds, average=\"macro\") if len(all_targets) > 0 else 0.0\n",
        "\n",
        "    return epoch_loss, epoch_acc, epoch_f1, all_targets, all_preds\n",
        "\n",
        "def create_model_and_optim(model_name, num_classes, lr=3e-4, weight_decay=1e-4, device=DEVICE):\n",
        "    model = timm.create_model(\n",
        "        model_name,\n",
        "        pretrained=True,\n",
        "        num_classes=num_classes\n",
        "    )\n",
        "    model.to(device)\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=20)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    return model, criterion, optimizer, scheduler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Training Loop for Common Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ready to train on 6 common classes\n"
          ]
        }
      ],
      "source": [
        "def train_model_common_classes(\n",
        "    model_name,\n",
        "    num_classes,\n",
        "    train_loader,\n",
        "    val_loaders_dict,  # Dictionary of validation loaders\n",
        "    max_epochs=20,\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4,\n",
        "    device=DEVICE,\n",
        "    early_stopping_patience=5\n",
        "):\n",
        "    print(f\"Starting training for model: {model_name}\")\n",
        "    print(f\"  -> Training on {num_classes} common classes\")\n",
        "    print(f\"  -> Validation sets: {list(val_loaders_dict.keys())}\")\n",
        "    \n",
        "    model, criterion, optimizer, scheduler = create_model_and_optim(\n",
        "        model_name=model_name,\n",
        "        num_classes=num_classes,\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    best_val_f1 = -1.0\n",
        "    best_state = None\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"train_f1\": [],\n",
        "    }\n",
        "    # Add validation history for each dataset\n",
        "    for dataset_name in val_loaders_dict.keys():\n",
        "        history[f\"val_{dataset_name}_loss\"] = []\n",
        "        history[f\"val_{dataset_name}_acc\"] = []\n",
        "        history[f\"val_{dataset_name}_f1\"] = []\n",
        "\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(1, max_epochs + 1):\n",
        "        start_time = time.time()\n",
        "\n",
        "        train_loss, train_acc, train_f1 = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, device\n",
        "        )\n",
        "        \n",
        "        # Evaluate on all validation sets (skip empty ones)\n",
        "        val_results = {}\n",
        "        for dataset_name, val_loader in val_loaders_dict.items():\n",
        "            # Skip empty validation loaders\n",
        "            if len(val_loader) == 0:\n",
        "                print(f\"  Warning: {dataset_name} validation set is empty, skipping...\")\n",
        "                val_results[dataset_name] = {\n",
        "                    \"loss\": 0.0,\n",
        "                    \"acc\": 0.0,\n",
        "                    \"f1\": 0.0\n",
        "                }\n",
        "            else:\n",
        "                val_loss, val_acc, val_f1, _, _ = evaluate(\n",
        "                    model, val_loader, criterion, device\n",
        "                )\n",
        "                val_results[dataset_name] = {\n",
        "                    \"loss\": val_loss,\n",
        "                    \"acc\": val_acc,\n",
        "                    \"f1\": val_f1\n",
        "                }\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update history\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"train_f1\"].append(train_f1)\n",
        "        \n",
        "        for dataset_name, results in val_results.items():\n",
        "            history[f\"val_{dataset_name}_loss\"].append(results[\"loss\"])\n",
        "            history[f\"val_{dataset_name}_acc\"].append(results[\"acc\"])\n",
        "            history[f\"val_{dataset_name}_f1\"].append(results[\"f1\"])\n",
        "\n",
        "        # Use average F1 across all non-empty validation sets for best model selection\n",
        "        non_empty_f1s = [r[\"f1\"] for r in val_results.values() if r[\"f1\"] > 0.0]\n",
        "        if len(non_empty_f1s) > 0:\n",
        "            avg_val_f1 = np.mean(non_empty_f1s)\n",
        "        else:\n",
        "            avg_val_f1 = 0.0\n",
        "            print(\"  Warning: All validation sets are empty!\")\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch}/{max_epochs} ({elapsed:.1f}s)\")\n",
        "        print(f\"  Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f}\")\n",
        "        for dataset_name, results in val_results.items():\n",
        "            print(f\"  Val {dataset_name} - Loss: {results['loss']:.4f}, Acc: {results['acc']:.4f}, F1: {results['f1']:.4f}\")\n",
        "        print(f\"  Avg Val F1: {avg_val_f1:.4f}\")\n",
        "\n",
        "        if avg_val_f1 > best_val_f1:\n",
        "            best_val_f1 = avg_val_f1\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            epochs_without_improvement = 0\n",
        "            print(f\"  -> New best! Avg Val F1: {best_val_f1:.4f}\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= early_stopping_patience:\n",
        "                print(f\"  -> Early stopping after {epoch} epochs\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(best_state)\n",
        "    return model, history, best_val_f1\n",
        "\n",
        "# Prepare validation loaders dictionary\n",
        "val_loaders = {\n",
        "    \"main\": val_loader_main,\n",
        "    \"plant_doc\": val_loader_plant_doc,\n",
        "    \"fieldplant\": val_loader_fieldplant\n",
        "}\n",
        "\n",
        "NUM_COMMON_CLASSES = len(common_canonical_labels)\n",
        "print(f\"\\nReady to train on {NUM_COMMON_CLASSES} common classes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Train EfficientNet-B0 on Common Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TRAINING EFFICIENTNET-B0 ON COMMON CLASSES\n",
            "======================================================================\n",
            "Starting training for model: efficientnet_b0\n",
            "  -> Training on 6 common classes\n",
            "  -> Validation sets: ['main', 'plant_doc', 'fieldplant']\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 1/20 (512.0s)\n",
            "  Train - Loss: 1.8157, Acc: 0.3748, F1: 0.3694\n",
            "  Val main - Loss: 0.7480, Acc: 0.6652, F1: 0.5172\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 1.1761, Acc: 0.5363, F1: 0.1589\n",
            "  Avg Val F1: 0.3380\n",
            "  -> New best! Avg Val F1: 0.3380\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 2/20 (543.7s)\n",
            "  Train - Loss: 1.0689, Acc: 0.6299, F1: 0.6276\n",
            "  Val main - Loss: 0.3151, Acc: 0.8869, F1: 0.7983\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 1.2054, Acc: 0.5294, F1: 0.2941\n",
            "  Avg Val F1: 0.5462\n",
            "  -> New best! Avg Val F1: 0.5462\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 3/20 (1518.3s)\n",
            "  Train - Loss: 0.7964, Acc: 0.7175, F1: 0.7177\n",
            "  Val main - Loss: 0.2193, Acc: 0.9084, F1: 0.8347\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.6840, Acc: 0.7555, F1: 0.4062\n",
            "  Avg Val F1: 0.6205\n",
            "  -> New best! Avg Val F1: 0.6205\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 4/20 (1493.4s)\n",
            "  Train - Loss: 0.6568, Acc: 0.7706, F1: 0.7708\n",
            "  Val main - Loss: 0.2762, Acc: 0.9048, F1: 0.7956\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.7362, Acc: 0.7486, F1: 0.4153\n",
            "  Avg Val F1: 0.6055\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 5/20 (1464.7s)\n",
            "  Train - Loss: 0.5752, Acc: 0.7971, F1: 0.7967\n",
            "  Val main - Loss: 0.1915, Acc: 0.9201, F1: 0.8425\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.6445, Acc: 0.7589, F1: 0.4085\n",
            "  Avg Val F1: 0.6255\n",
            "  -> New best! Avg Val F1: 0.6255\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 6/20 (1563.3s)\n",
            "  Train - Loss: 0.5326, Acc: 0.8152, F1: 0.8171\n",
            "  Val main - Loss: 0.1537, Acc: 0.9300, F1: 0.8675\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.5653, Acc: 0.8258, F1: 0.4667\n",
            "  Avg Val F1: 0.6671\n",
            "  -> New best! Avg Val F1: 0.6671\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 7/20 (1186.3s)\n",
            "  Train - Loss: 0.4777, Acc: 0.8312, F1: 0.8323\n",
            "  Val main - Loss: 0.1389, Acc: 0.9515, F1: 0.9180\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.4302, Acc: 0.8547, F1: 0.5041\n",
            "  Avg Val F1: 0.7111\n",
            "  -> New best! Avg Val F1: 0.7111\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 8/20 (928.8s)\n",
            "  Train - Loss: 0.4313, Acc: 0.8402, F1: 0.8408\n",
            "  Val main - Loss: 0.1790, Acc: 0.9291, F1: 0.8916\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.5540, Acc: 0.8062, F1: 0.5199\n",
            "  Avg Val F1: 0.7058\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 9/20 (1015.4s)\n",
            "  Train - Loss: 0.3880, Acc: 0.8609, F1: 0.8606\n",
            "  Val main - Loss: 0.1320, Acc: 0.9488, F1: 0.9126\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.4084, Acc: 0.8547, F1: 0.5479\n",
            "  Avg Val F1: 0.7302\n",
            "  -> New best! Avg Val F1: 0.7302\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 10/20 (1264.8s)\n",
            "  Train - Loss: 0.3795, Acc: 0.8639, F1: 0.8641\n",
            "  Val main - Loss: 0.1455, Acc: 0.9488, F1: 0.9098\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.3470, Acc: 0.8800, F1: 0.5821\n",
            "  Avg Val F1: 0.7459\n",
            "  -> New best! Avg Val F1: 0.7459\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 11/20 (1557.6s)\n",
            "  Train - Loss: 0.3512, Acc: 0.8745, F1: 0.8748\n",
            "  Val main - Loss: 0.1303, Acc: 0.9461, F1: 0.9045\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.3306, Acc: 0.8858, F1: 0.6287\n",
            "  Avg Val F1: 0.7666\n",
            "  -> New best! Avg Val F1: 0.7666\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 12/20 (1116.8s)\n",
            "  Train - Loss: 0.3270, Acc: 0.8813, F1: 0.8819\n",
            "  Val main - Loss: 0.1194, Acc: 0.9452, F1: 0.9022\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.4094, Acc: 0.8639, F1: 0.5758\n",
            "  Avg Val F1: 0.7390\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 13/20 (783.3s)\n",
            "  Train - Loss: 0.3071, Acc: 0.8875, F1: 0.8878\n",
            "  Val main - Loss: 0.1421, Acc: 0.9461, F1: 0.8987\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.3526, Acc: 0.8824, F1: 0.6595\n",
            "  Avg Val F1: 0.7791\n",
            "  -> New best! Avg Val F1: 0.7791\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 14/20 (799.5s)\n",
            "  Train - Loss: 0.2769, Acc: 0.8987, F1: 0.8988\n",
            "  Val main - Loss: 0.1339, Acc: 0.9497, F1: 0.9081\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.2787, Acc: 0.9135, F1: 0.7387\n",
            "  Avg Val F1: 0.8234\n",
            "  -> New best! Avg Val F1: 0.8234\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 15/20 (896.6s)\n",
            "  Train - Loss: 0.2678, Acc: 0.9016, F1: 0.9017\n",
            "  Val main - Loss: 0.1239, Acc: 0.9479, F1: 0.9106\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.2662, Acc: 0.9146, F1: 0.7312\n",
            "  Avg Val F1: 0.8209\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 16/20 (526.3s)\n",
            "  Train - Loss: 0.2426, Acc: 0.9097, F1: 0.9107\n",
            "  Val main - Loss: 0.0949, Acc: 0.9623, F1: 0.9327\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.2412, Acc: 0.9204, F1: 0.7910\n",
            "  Avg Val F1: 0.8618\n",
            "  -> New best! Avg Val F1: 0.8618\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 17/20 (834.6s)\n",
            "  Train - Loss: 0.2241, Acc: 0.9153, F1: 0.9153\n",
            "  Val main - Loss: 0.1070, Acc: 0.9569, F1: 0.9210\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.2321, Acc: 0.9262, F1: 0.7845\n",
            "  Avg Val F1: 0.8528\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 18/20 (1787.2s)\n",
            "  Train - Loss: 0.2206, Acc: 0.9158, F1: 0.9165\n",
            "  Val main - Loss: 0.0891, Acc: 0.9668, F1: 0.9408\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.1895, Acc: 0.9400, F1: 0.8187\n",
            "  Avg Val F1: 0.8798\n",
            "  -> New best! Avg Val F1: 0.8798\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 19/20 (1452.1s)\n",
            "  Train - Loss: 0.2212, Acc: 0.9198, F1: 0.9201\n",
            "  Val main - Loss: 0.0898, Acc: 0.9632, F1: 0.9354\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.1941, Acc: 0.9412, F1: 0.8143\n",
            "  Avg Val F1: 0.8748\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 20/20 (1039.7s)\n",
            "  Train - Loss: 0.2103, Acc: 0.9232, F1: 0.9236\n",
            "  Val main - Loss: 0.0846, Acc: 0.9668, F1: 0.9397\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.1915, Acc: 0.9423, F1: 0.8245\n",
            "  Avg Val F1: 0.8821\n",
            "  -> New best! Avg Val F1: 0.8821\n",
            "\n",
            "Model saved to: models\\efficientnet_b0_common_classes.pt\n"
          ]
        }
      ],
      "source": [
        "# Train EfficientNet-B0\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING EFFICIENTNET-B0 ON COMMON CLASSES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "model_eff, history_eff, best_val_f1_eff = train_model_common_classes(\n",
        "    model_name=\"efficientnet_b0\",\n",
        "    num_classes=NUM_COMMON_CLASSES,\n",
        "    train_loader=train_loader,\n",
        "    val_loaders_dict=val_loaders,\n",
        "    max_epochs=20,\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4,\n",
        "    device=DEVICE,\n",
        "    early_stopping_patience=5\n",
        ")\n",
        "\n",
        "# Save model\n",
        "checkpoint_eff = {\n",
        "    \"model_name\": \"efficientnet_b0\",\n",
        "    \"num_classes\": NUM_COMMON_CLASSES,\n",
        "    \"state_dict\": model_eff.state_dict(),\n",
        "    \"best_val_f1\": best_val_f1_eff,\n",
        "    \"history\": history_eff,\n",
        "    \"common_classes\": common_canonical_labels,\n",
        "    \"common_class_ids_original\": common_class_ids_original,\n",
        "}\n",
        "\n",
        "torch.save(checkpoint_eff, MODELS_DIR / \"efficientnet_b0_common_classes.pt\")\n",
        "print(f\"\\nModel saved to: {MODELS_DIR / 'efficientnet_b0_common_classes.pt'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Train ViT-Base on Common Classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TRAINING VIT-BASE ON COMMON CLASSES\n",
            "======================================================================\n",
            "Starting training for model: vit_base_patch16_224\n",
            "  -> Training on 6 common classes\n",
            "  -> Validation sets: ['main', 'plant_doc', 'fieldplant']\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 1/20 (1963.2s)\n",
            "  Train - Loss: 1.3613, Acc: 0.4681, F1: 0.4649\n",
            "  Val main - Loss: 0.6732, Acc: 0.7666, F1: 0.6592\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.9760, Acc: 0.6401, F1: 0.2155\n",
            "  Avg Val F1: 0.4373\n",
            "  -> New best! Avg Val F1: 0.4373\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 2/20 (2080.3s)\n",
            "  Train - Loss: 0.9481, Acc: 0.6560, F1: 0.6500\n",
            "  Val main - Loss: 0.5450, Acc: 0.7513, F1: 0.6021\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.8492, Acc: 0.6909, F1: 0.2305\n",
            "  Avg Val F1: 0.4163\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 3/20 (2200.0s)\n",
            "  Train - Loss: 0.8474, Acc: 0.6924, F1: 0.6921\n",
            "  Val main - Loss: 0.3658, Acc: 0.8402, F1: 0.7101\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.8200, Acc: 0.7370, F1: 0.2568\n",
            "  Avg Val F1: 0.4835\n",
            "  -> New best! Avg Val F1: 0.4835\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 4/20 (2275.9s)\n",
            "  Train - Loss: 0.7818, Acc: 0.7133, F1: 0.7106\n",
            "  Val main - Loss: 0.4707, Acc: 0.8097, F1: 0.7163\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.7594, Acc: 0.7532, F1: 0.2507\n",
            "  Avg Val F1: 0.4835\n",
            "  -> New best! Avg Val F1: 0.4835\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 5/20 (2364.7s)\n",
            "  Train - Loss: 0.7519, Acc: 0.7244, F1: 0.7225\n",
            "  Val main - Loss: 0.4773, Acc: 0.8025, F1: 0.7357\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.7719, Acc: 0.7128, F1: 0.2800\n",
            "  Avg Val F1: 0.5078\n",
            "  -> New best! Avg Val F1: 0.5078\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 6/20 (2517.5s)\n",
            "  Train - Loss: 0.7192, Acc: 0.7337, F1: 0.7309\n",
            "  Val main - Loss: 0.2462, Acc: 0.8923, F1: 0.8161\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.5993, Acc: 0.8166, F1: 0.3404\n",
            "  Avg Val F1: 0.5783\n",
            "  -> New best! Avg Val F1: 0.5783\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 7/20 (2741.7s)\n",
            "  Train - Loss: 0.6790, Acc: 0.7478, F1: 0.7491\n",
            "  Val main - Loss: 0.3606, Acc: 0.8609, F1: 0.7763\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.8324, Acc: 0.7313, F1: 0.2976\n",
            "  Avg Val F1: 0.5370\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 8/20 (2852.8s)\n",
            "  Train - Loss: 0.6013, Acc: 0.7756, F1: 0.7761\n",
            "  Val main - Loss: 0.4010, Acc: 0.8097, F1: 0.7021\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.7064, Acc: 0.7624, F1: 0.3477\n",
            "  Avg Val F1: 0.5249\n",
            "  Warning: plant_doc validation set is empty, skipping...\n",
            "Epoch 9/20 (22323.5s)\n",
            "  Train - Loss: 0.5555, Acc: 0.7894, F1: 0.7902\n",
            "  Val main - Loss: 0.2658, Acc: 0.8815, F1: 0.7779\n",
            "  Val plant_doc - Loss: 0.0000, Acc: 0.0000, F1: 0.0000\n",
            "  Val fieldplant - Loss: 0.5920, Acc: 0.7912, F1: 0.4423\n",
            "  Avg Val F1: 0.6101\n",
            "  -> New best! Avg Val F1: 0.6101\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTRAINING VIT-BASE ON COMMON CLASSES\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m model_vit, history_vit, best_val_f1_vit \u001b[38;5;241m=\u001b[39m train_model_common_classes(\n\u001b[0;32m      7\u001b[0m     model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_base_patch16_224\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39mNUM_COMMON_CLASSES,\n\u001b[0;32m      9\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m     10\u001b[0m     val_loaders_dict\u001b[38;5;241m=\u001b[39mval_loaders,\n\u001b[0;32m     11\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     12\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m,\n\u001b[0;32m     13\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m,\n\u001b[0;32m     14\u001b[0m     device\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[0;32m     15\u001b[0m     early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[0;32m     19\u001b[0m checkpoint_vit \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_base_patch16_224\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_classes\u001b[39m\u001b[38;5;124m\"\u001b[39m: NUM_COMMON_CLASSES,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommon_class_ids_original\u001b[39m\u001b[38;5;124m\"\u001b[39m: common_class_ids_original,\n\u001b[0;32m     27\u001b[0m }\n",
            "Cell \u001b[1;32mIn[12], line 42\u001b[0m, in \u001b[0;36mtrain_model_common_classes\u001b[1;34m(model_name, num_classes, train_loader, val_loaders_dict, max_epochs, lr, weight_decay, device, early_stopping_patience)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, max_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     40\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 42\u001b[0m     train_loss, train_acc, train_f1 \u001b[38;5;241m=\u001b[39m train_one_epoch(\n\u001b[0;32m     43\u001b[0m         model, train_loader, criterion, optimizer, device\n\u001b[0;32m     44\u001b[0m     )\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Evaluate on all validation sets (skip empty ones)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     val_results \u001b[38;5;241m=\u001b[39m {}\n",
            "Cell \u001b[1;32mIn[9], line 12\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\timm\\models\\vision_transformer.py:853\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 853\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_features(x)\n\u001b[0;32m    854\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[0;32m    855\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\timm\\models\\vision_transformer.py:834\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    832\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 834\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x)\n\u001b[0;32m    835\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\timm\\models\\vision_transformer.py:169\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 169\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x))))\n\u001b[0;32m    170\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))))\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\Kero\\anaconda3\\Lib\\site-packages\\timm\\models\\vision_transformer.py:93\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     90\u001b[0m q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_norm(q), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_norm(k)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfused_attn:\n\u001b[1;32m---> 93\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m     94\u001b[0m         q, k, v,\n\u001b[0;32m     95\u001b[0m         dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_drop\u001b[38;5;241m.\u001b[39mp \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.\u001b[39m,\n\u001b[0;32m     96\u001b[0m     )\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     q \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train ViT-Base\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING VIT-BASE ON COMMON CLASSES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "model_vit, history_vit, best_val_f1_vit = train_model_common_classes(\n",
        "    model_name=\"vit_base_patch16_224\",\n",
        "    num_classes=NUM_COMMON_CLASSES,\n",
        "    train_loader=train_loader,\n",
        "    val_loaders_dict=val_loaders,\n",
        "    max_epochs=20,\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4,\n",
        "    device=DEVICE,\n",
        "    early_stopping_patience=5\n",
        ")\n",
        "\n",
        "# Save model\n",
        "checkpoint_vit = {\n",
        "    \"model_name\": \"vit_base_patch16_224\",\n",
        "    \"num_classes\": NUM_COMMON_CLASSES,\n",
        "    \"state_dict\": model_vit.state_dict(),\n",
        "    \"best_val_f1\": best_val_f1_vit,\n",
        "    \"history\": history_vit,\n",
        "    \"common_classes\": common_canonical_labels,\n",
        "    \"common_class_ids_original\": common_class_ids_original,\n",
        "}\n",
        "\n",
        "torch.save(checkpoint_vit, MODELS_DIR / \"vit_base_patch16_224_common_classes.pt\")\n",
        "print(f\"\\nModel saved to: {MODELS_DIR / 'vit_base_patch16_224_common_classes.pt'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Validation Block - Performance Statistics on All Three Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_on_all_datasets(model, val_loaders_dict, criterion, device, model_name):\n",
        "    \"\"\"Evaluate model on all validation datasets and return detailed statistics.\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"VALIDATION RESULTS FOR {model_name.upper()}\")\n",
        "    print(f\"{'='*70}\\n\")\n",
        "    \n",
        "    for dataset_name, val_loader in val_loaders_dict.items():\n",
        "        print(f\"Evaluating on {dataset_name.upper()} dataset...\")\n",
        "        val_loss, val_acc, val_f1, all_targets, all_preds = evaluate(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "        \n",
        "        # Per-class metrics\n",
        "        class_report = classification_report(\n",
        "            all_targets, all_preds,\n",
        "            target_names=[common_canonical_labels[i] for i in range(NUM_COMMON_CLASSES)],\n",
        "            output_dict=True,\n",
        "            zero_division=0\n",
        "        )\n",
        "        \n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(all_targets, all_preds)\n",
        "        \n",
        "        results[dataset_name] = {\n",
        "            \"loss\": val_loss,\n",
        "            \"accuracy\": val_acc,\n",
        "            \"f1_macro\": val_f1,\n",
        "            \"all_targets\": all_targets,\n",
        "            \"all_preds\": all_preds,\n",
        "            \"classification_report\": class_report,\n",
        "            \"confusion_matrix\": cm\n",
        "        }\n",
        "        \n",
        "        print(f\"  Loss: {val_loss:.4f}\")\n",
        "        print(f\"  Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "        print(f\"  Macro F1: {val_f1:.4f} ({val_f1*100:.2f}%)\")\n",
        "        print(f\"  Total samples: {len(all_targets)}\")\n",
        "        print()\n",
        "        \n",
        "        # Per-class accuracy\n",
        "        print(f\"  Per-class accuracy on {dataset_name}:\")\n",
        "        for class_id in range(NUM_COMMON_CLASSES):\n",
        "            class_mask = all_targets == class_id\n",
        "            if np.sum(class_mask) > 0:\n",
        "                class_acc = (all_preds[class_mask] == class_id).mean()\n",
        "                class_name = common_canonical_labels[class_id]\n",
        "                print(f\"    Class {class_id} ({class_name}): {class_acc:.4f} ({np.sum(class_mask)} samples)\")\n",
        "        print()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Evaluate EfficientNet\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EFFICIENTNET-B0 VALIDATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "results_eff = evaluate_on_all_datasets(model_eff, val_loaders, criterion, DEVICE, \"EfficientNet-B0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate ViT\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VIT-BASE VALIDATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "results_vit = evaluate_on_all_datasets(model_vit, val_loaders, criterion, DEVICE, \"ViT-Base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Summary Statistics and Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary comparison table\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY COMPARISON TABLE\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nPerformance across all validation datasets:\\n\")\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_data = []\n",
        "for dataset_name in [\"main\", \"plant_doc\", \"fieldplant\"]:\n",
        "    comparison_data.append({\n",
        "        \"Dataset\": dataset_name.upper(),\n",
        "        \"Model\": \"EfficientNet-B0\",\n",
        "        \"Accuracy\": results_eff[dataset_name][\"accuracy\"],\n",
        "        \"F1 Macro\": results_eff[dataset_name][\"f1_macro\"],\n",
        "        \"Loss\": results_eff[dataset_name][\"loss\"],\n",
        "        \"Samples\": len(results_eff[dataset_name][\"all_targets\"])\n",
        "    })\n",
        "    comparison_data.append({\n",
        "        \"Dataset\": dataset_name.upper(),\n",
        "        \"Model\": \"ViT-Base\",\n",
        "        \"Accuracy\": results_vit[dataset_name][\"accuracy\"],\n",
        "        \"F1 Macro\": results_vit[dataset_name][\"f1_macro\"],\n",
        "        \"Loss\": results_vit[dataset_name][\"loss\"],\n",
        "        \"Samples\": len(results_vit[dataset_name][\"all_targets\"])\n",
        "    })\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "print(df_comparison.to_string(index=False))\n",
        "\n",
        "# Overall statistics\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"OVERALL STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nEfficientNet-B0:\")\n",
        "print(f\"  Average Accuracy across datasets: {df_comparison[df_comparison['Model']=='EfficientNet-B0']['Accuracy'].mean():.4f}\")\n",
        "print(f\"  Average F1 Macro across datasets: {df_comparison[df_comparison['Model']=='EfficientNet-B0']['F1 Macro'].mean():.4f}\")\n",
        "\n",
        "print(\"\\nViT-Base:\")\n",
        "print(f\"  Average Accuracy across datasets: {df_comparison[df_comparison['Model']=='ViT-Base']['Accuracy'].mean():.4f}\")\n",
        "print(f\"  Average F1 Macro across datasets: {df_comparison[df_comparison['Model']=='ViT-Base']['F1 Macro'].mean():.4f}\")\n",
        "\n",
        "# Best model per dataset\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"BEST MODEL PER DATASET\")\n",
        "print(\"=\"*70)\n",
        "for dataset_name in [\"main\", \"plant_doc\", \"fieldplant\"]:\n",
        "    eff_f1 = results_eff[dataset_name][\"f1_macro\"]\n",
        "    vit_f1 = results_vit[dataset_name][\"f1_macro\"]\n",
        "    best_model = \"EfficientNet-B0\" if eff_f1 > vit_f1 else \"ViT-Base\"\n",
        "    print(f\"{dataset_name.upper()}: {best_model} (F1: {max(eff_f1, vit_f1):.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14: Detailed Per-Class Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed per-class analysis\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DETAILED PER-CLASS ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for dataset_name in [\"main\", \"plant_doc\", \"fieldplant\"]:\n",
        "    print(f\"\\n{dataset_name.upper()} Dataset:\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    for model_name, results in [(\"EfficientNet-B0\", results_eff), (\"ViT-Base\", results_vit)]:\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        all_targets = results[dataset_name][\"all_targets\"]\n",
        "        all_preds = results[dataset_name][\"all_preds\"]\n",
        "        \n",
        "        for class_id in range(NUM_COMMON_CLASSES):\n",
        "            class_name = common_canonical_labels[class_id]\n",
        "            class_mask = all_targets == class_id\n",
        "            if np.sum(class_mask) > 0:\n",
        "                class_acc = (all_preds[class_mask] == class_id).mean()\n",
        "                class_samples = np.sum(class_mask)\n",
        "                class_correct = np.sum((all_preds[class_mask] == class_id))\n",
        "                \n",
        "                # Precision, Recall, F1 from classification report\n",
        "                if class_name in results[dataset_name][\"classification_report\"]:\n",
        "                    metrics = results[dataset_name][\"classification_report\"][class_name]\n",
        "                    precision = metrics.get(\"precision\", 0)\n",
        "                    recall = metrics.get(\"recall\", 0)\n",
        "                    f1 = metrics.get(\"f1-score\", 0)\n",
        "                else:\n",
        "                    precision = recall = f1 = 0\n",
        "                \n",
        "                print(f\"  {class_name}:\")\n",
        "                print(f\"    Accuracy: {class_acc:.4f} ({class_correct}/{class_samples})\")\n",
        "                print(f\"    Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
